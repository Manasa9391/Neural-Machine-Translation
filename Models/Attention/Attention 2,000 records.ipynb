{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences = pd.read_csv(r\"s3://cognitivecomputing/Finalproject/french_100000.csv\")\n",
    "english_sentences = pd.read_csv(r\"s3://cognitivecomputing/Finalproject/english_100000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English_sentences'], dtype='object')\n",
      "Index(['French_sentences'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "english_sentences = english_sentences.iloc[:,-1:]\n",
    "french_sentences = french_sentences.iloc[:,-1:]\n",
    "print(english_sentences.columns)\n",
    "print(french_sentences.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "french_sample = french_sentences.iloc[:n,:]\n",
    "english_sample = english_sentences.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "english_sample.columns\n",
    "english_sample['English_sentences']\n",
    "for i in range(len(english_sample['English_sentences'].index)):\n",
    "    if type(english_sample['English_sentences'][i]) != str:\n",
    "        english_sample['English_sentences'][i] = str(english_sample['English_sentences'][i])\n",
    "        \n",
    "french_sample['French_sentences']\n",
    "for i in range(len(french_sample['French_sentences'].index)):\n",
    "    if type(french_sample['French_sentences'][i]) != str:\n",
    "        french_sample['French_sentences'][i] = str(french_sample['French_sentences'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def cleaning_sentence(data, column):\n",
    "    sentence = [preprocess_sentence(i) for i in data[column]]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():    \n",
    "    def __init__(self, lang):        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "french_sent = cleaning_sentence(french_sample, 'French_sentences')\n",
    "english_sent = cleaning_sentence(english_sample, 'English_sentences')\n",
    "\n",
    "# index language using the class defined above\n",
    "inp_lang = LanguageIndex(en for en in english_sent)\n",
    "targ_lang = LanguageIndex(fr for fr in french_sent)\n",
    "\n",
    "#English sentences\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en in english_sent]\n",
    "#French Sentences\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in fr.split(' ')] for fr in french_sent]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "# Padding the input and output tensor to the maximum length\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                              maxlen=max_length_tar, \n",
    "                                                              padding='post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1600, 400, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "64\n",
      "25\n",
      "4836\n",
      "6375\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "print(BUFFER_SIZE)\n",
    "print(BATCH_SIZE)\n",
    "print(N_BATCH)\n",
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 128), (64, 136)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                           return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_activation='sigmoid', \n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.6133\n",
      "Epoch 1 Loss 1.4305\n",
      "Time taken for 1 epoch 801.974363565445 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.2860\n",
      "Epoch 2 Loss 1.3408\n",
      "Time taken for 1 epoch 794.9118905067444 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3198\n",
      "Epoch 3 Loss 1.3330\n",
      "Time taken for 1 epoch 794.8264193534851 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1602\n",
      "Epoch 4 Loss 1.3250\n",
      "Time taken for 1 epoch 796.3854467868805 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2609\n",
      "Epoch 5 Loss 1.3072\n",
      "Time taken for 1 epoch 795.9920034408569 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2260\n",
      "Epoch 6 Loss 1.2804\n",
      "Time taken for 1 epoch 798.8389747142792 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0170\n",
      "Epoch 7 Loss 1.2528\n",
      "Time taken for 1 epoch 795.4572684764862 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2021\n",
      "Epoch 8 Loss 1.2250\n",
      "Time taken for 1 epoch 795.9820351600647 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1612\n",
      "Epoch 9 Loss 1.1991\n",
      "Time taken for 1 epoch 796.1839380264282 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.3802\n",
      "Epoch 10 Loss 1.1767\n",
      "Time taken for 1 epoch 795.8377313613892 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.1180\n",
      "Epoch 11 Loss 1.1563\n",
      "Time taken for 1 epoch 796.8944675922394 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0500\n",
      "Epoch 12 Loss 1.1370\n",
      "Time taken for 1 epoch 795.3559184074402 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.1215\n",
      "Epoch 13 Loss 1.1192\n",
      "Time taken for 1 epoch 796.2954330444336 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.9920\n",
      "Epoch 14 Loss 1.1021\n",
      "Time taken for 1 epoch 795.7595751285553 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.0994\n",
      "Epoch 15 Loss 1.0865\n",
      "Time taken for 1 epoch 796.0520932674408 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.1382\n",
      "Epoch 16 Loss 1.0701\n",
      "Time taken for 1 epoch 797.2612671852112 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.0489\n",
      "Epoch 17 Loss 1.0554\n",
      "Time taken for 1 epoch 795.3063824176788 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.0205\n",
      "Epoch 18 Loss 1.0395\n",
      "Time taken for 1 epoch 795.6080045700073 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.0874\n",
      "Epoch 19 Loss 1.0261\n",
      "Time taken for 1 epoch 795.2623040676117 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0613\n",
      "Epoch 20 Loss 1.0123\n",
      "Time taken for 1 epoch 796.4888684749603 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.0882\n",
      "Epoch 21 Loss 0.9992\n",
      "Time taken for 1 epoch 795.6857967376709 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.9359\n",
      "Epoch 22 Loss 0.9866\n",
      "Time taken for 1 epoch 795.9608097076416 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.9191\n",
      "Epoch 23 Loss 0.9751\n",
      "Time taken for 1 epoch 796.131130695343 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.8468\n",
      "Epoch 24 Loss 0.9639\n",
      "Time taken for 1 epoch 846.7596046924591 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.9309\n",
      "Epoch 25 Loss 0.9532\n",
      "Time taken for 1 epoch 796.1853892803192 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))   \n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resumption of the session'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sample.iloc[:1,:].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> resumption of the session <end>\n",
      "Predicted translation: monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur le president monsieur \n"
     ]
    }
   ],
   "source": [
    "translate(english_sample.iloc[:1,:].values[0][0], encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
