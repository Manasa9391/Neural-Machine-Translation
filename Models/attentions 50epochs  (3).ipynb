{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data \n",
    "french_sentences = pd.read_csv(r\"s3://cognitivecomputingbucket/french_100000.csv\") \n",
    "english_sentences = pd.read_csv(r\"s3://cognitivecomputingbucket/english_100000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English_sentences'], dtype='object')\n",
      "Index(['French_sentences'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#takign only sentence columns \n",
    "english_sentences = english_sentences.iloc[:,-1:]\n",
    "french_sentences = french_sentences.iloc[:,-1:]\n",
    "print(english_sentences.columns)\n",
    "print(french_sentences.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking 1000 sample of sentences \n",
    "n = 1000\n",
    "french_sample = french_sentences.iloc[:n,:]\n",
    "english_sample = english_sentences.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Takign only sentences which are strings \n",
    "english_sample.columns\n",
    "english_sample['English_sentences']\n",
    "for i in range(len(english_sample['English_sentences'].index)):\n",
    "    if type(english_sample['English_sentences'][i]) != str:\n",
    "        english_sample['English_sentences'][i] = str(english_sample['English_sentences'][i])\n",
    "        \n",
    "french_sample['French_sentences']\n",
    "for i in range(len(french_sample['French_sentences'].index)):\n",
    "    if type(french_sample['French_sentences'][i]) != str:\n",
    "        french_sample['French_sentences'][i] = str(french_sample['French_sentences'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the spaces---Stripping the sentence\n",
    "#making the sentence lower case\n",
    "#Appending each sentence with start and end keyword\n",
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "#Applying the pre-processing steps to all the sentences\n",
    "def cleaning_sentence(data, column):\n",
    "    sentence = [preprocess_sentence(i) for i in data[column]]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the class to get sentence indexex which has word to index and index to word\n",
    "class LanguageIndex():    \n",
    "    def __init__(self, lang):        \n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "          self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "          self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the maximum lenght of sentences\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Sentences\n",
    "french_sent = cleaning_sentence(french_sample, 'French_sentences')\n",
    "english_sent = cleaning_sentence(english_sample, 'English_sentences')\n",
    "\n",
    "# index language using the class defined above\n",
    "inp_lang = LanguageIndex(en for en in english_sent)\n",
    "targ_lang = LanguageIndex(fr for fr in french_sent)\n",
    "\n",
    "#English sentences\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en in english_sent]\n",
    "#French Sentences\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in fr.split(' ')] for fr in french_sent]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "\n",
    "# Padding the input and output tensor to the maximum length\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                             maxlen=max_length_inp,\n",
    "                                                             padding='post')\n",
    "\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                              maxlen=max_length_tar, \n",
    "                                                              padding='post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the sentence train and test split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800, 200, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length of input and validation sentences\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "64\n",
      "12\n",
      "3343\n",
      "4290\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train) #Buffer size\n",
    "BATCH_SIZE = 64 #Batch size \n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256 #Embedding dimension\n",
    "units = 1024 #GRU units  \n",
    "vocab_inp_size = len(inp_lang.word2idx) #Vocabulary size of English sentences\n",
    "vocab_tar_size = len(targ_lang.word2idx)#Vocabulary size of french sentences\n",
    "\n",
    "\n",
    "print(BUFFER_SIZE)\n",
    "print(BATCH_SIZE)\n",
    "print(N_BATCH)\n",
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 117), (64, 135)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU layer\n",
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                           return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_activation='sigmoid', \n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder layer\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Layer\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "      \n",
    "        #Scores calculations\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape---applying softmax\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector calculation\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))     \n",
    "       \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the hidden states of encoder \n",
    "def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder and decoder layer \n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer function--Adam optimizer \n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "#Loss sparse softmax cross entropy with logit \n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9022\n",
      "Epoch 1 Loss 1.5253\n",
      "Time taken for 1 epoch 362.0095627307892 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5867\n",
      "Epoch 2 Loss 1.4084\n",
      "Time taken for 1 epoch 359.5380024909973 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3944\n",
      "Epoch 3 Loss 1.3854\n",
      "Time taken for 1 epoch 364.6735143661499 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.3632\n",
      "Epoch 4 Loss 1.3727\n",
      "Time taken for 1 epoch 363.55119585990906 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4961\n",
      "Epoch 5 Loss 1.3799\n",
      "Time taken for 1 epoch 362.4090211391449 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1157\n",
      "Epoch 14 Loss 1.1711\n",
      "Time taken for 1 epoch 363.65593552589417 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.1292\n",
      "Epoch 15 Loss 1.1334\n",
      "Time taken for 1 epoch 361.9164071083069 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.9567\n",
      "Epoch 16 Loss 1.1213\n",
      "Time taken for 1 epoch 376.57103967666626 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.1284\n",
      "Epoch 17 Loss 1.0882\n",
      "Time taken for 1 epoch 359.595495223999 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.2492\n",
      "Epoch 18 Loss 1.0711\n",
      "Time taken for 1 epoch 367.5085361003876 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.9386\n",
      "Epoch 19 Loss 1.0457\n",
      "Time taken for 1 epoch 354.72379088401794 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.9604\n",
      "Epoch 20 Loss 1.0261\n",
      "Time taken for 1 epoch 357.08134174346924 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.0427\n",
      "Epoch 21 Loss 1.0005\n",
      "Time taken for 1 epoch 358.96389389038086 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.1093\n",
      "Epoch 22 Loss 0.9849\n",
      "Time taken for 1 epoch 359.4319500923157 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.9571\n",
      "Epoch 23 Loss 0.9592\n",
      "Time taken for 1 epoch 360.8645396232605 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.8809\n",
      "Epoch 24 Loss 0.9409\n",
      "Time taken for 1 epoch 357.011216878891 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.9294\n",
      "Epoch 25 Loss 0.9216\n",
      "Time taken for 1 epoch 357.5087504386902 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.8775\n",
      "Epoch 26 Loss 0.8974\n",
      "Time taken for 1 epoch 358.5355007648468 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.8661\n",
      "Epoch 27 Loss 0.8828\n",
      "Time taken for 1 epoch 358.99237990379333 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.9405\n",
      "Epoch 28 Loss 0.8575\n",
      "Time taken for 1 epoch 358.3963267803192 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.8452\n",
      "Epoch 29 Loss 0.8504\n",
      "Time taken for 1 epoch 357.6134605407715 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.8116\n",
      "Epoch 30 Loss 0.8382\n",
      "Time taken for 1 epoch 362.508095741272 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.8452\n",
      "Epoch 31 Loss 0.8173\n",
      "Time taken for 1 epoch 360.2413640022278 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.7496\n",
      "Epoch 32 Loss 0.8043\n",
      "Time taken for 1 epoch 356.9812912940979 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.7662\n",
      "Epoch 33 Loss 0.7835\n",
      "Time taken for 1 epoch 357.5076584815979 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.8351\n",
      "Epoch 34 Loss 0.7717\n",
      "Time taken for 1 epoch 357.69806838035583 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.7714\n",
      "Epoch 35 Loss 0.7564\n",
      "Time taken for 1 epoch 357.7179055213928 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.7299\n",
      "Epoch 36 Loss 0.7385\n",
      "Time taken for 1 epoch 361.58432960510254 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.7479\n",
      "Epoch 37 Loss 0.7192\n",
      "Time taken for 1 epoch 365.2639105319977 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.6921\n",
      "Epoch 38 Loss 0.7112\n",
      "Time taken for 1 epoch 367.39033794403076 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.7723\n",
      "Epoch 39 Loss 0.6939\n",
      "Time taken for 1 epoch 366.37653160095215 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.6248\n",
      "Epoch 40 Loss 0.6817\n",
      "Time taken for 1 epoch 367.84893107414246 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.7385\n",
      "Epoch 41 Loss 0.6633\n",
      "Time taken for 1 epoch 364.8924798965454 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.6433\n",
      "Epoch 42 Loss 0.6508\n",
      "Time taken for 1 epoch 365.6994345188141 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.5232\n",
      "Epoch 43 Loss 0.6377\n",
      "Time taken for 1 epoch 366.0913624763489 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.5701\n",
      "Epoch 44 Loss 0.6270\n",
      "Time taken for 1 epoch 364.08599305152893 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.6184\n",
      "Epoch 45 Loss 0.6153\n",
      "Time taken for 1 epoch 365.2207705974579 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.5125\n",
      "Epoch 46 Loss 0.5934\n",
      "Time taken for 1 epoch 366.4264249801636 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.5553\n",
      "Epoch 47 Loss 0.5878\n",
      "Time taken for 1 epoch 363.1919438838959 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.5975\n",
      "Epoch 48 Loss 0.5751\n",
      "Time taken for 1 epoch 363.9443163871765 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.5858\n",
      "Epoch 49 Loss 0.5584\n",
      "Time taken for 1 epoch 365.9274830818176 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.5210\n",
      "Epoch 50 Loss 0.5434\n",
      "Time taken for 1 epoch 364.3299813270569 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training the model for 50 epochs \n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    #Hidden states initialization\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    #epoch starts\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))   \n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translating the predicted sentences\n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> resumption of the session <end>\n",
      "Predicted translation: il sagit de la commission <end> \n"
     ]
    }
   ],
   "source": [
    "translate(english_sample.iloc[:1,:].values[0][0], encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DATASCIENCE]",
   "language": "python",
   "name": "conda-env-DATASCIENCE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
